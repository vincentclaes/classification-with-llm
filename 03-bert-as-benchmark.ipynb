{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbff05ae10750b97",
   "metadata": {},
   "source": [
    "# BERT as Benchmark\n",
    "\n",
    "- Fine tune BERT model using autotrain from Huggingface: https://huggingface.co/autotrain\n",
    "- Used the train.csv and val.csv in the autotrain job\n",
    "- Use default configuration\n",
    "- Fine-tuned model : https://huggingface.co/vincentclaes/autotrain-0br8k-gdjpm\n",
    "- accuracy: 0.9994418604651163\n",
    "\n",
    "## Run against test dataset and calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7485f556da24cea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T13:18:04.811483Z",
     "start_time": "2024-09-05T13:18:02.991191Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b7ce459bb0f4ac8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T08:42:37.942097Z",
     "start_time": "2024-09-06T08:41:54.309048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5368f8c02214e8aa0813c7ec16c30f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8965d40b696e478fbda13a13240f0350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6212e398e943b08d767f6f74319388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17df7d00d508419694a3ae63a0cb7f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2497c68781455cae72e277be751990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincent/Workspace/classification_with_llm/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the model (if you're using the model from the Hugging Face Hub)\n",
    "classifier_zero_shot = pipeline('zero-shot-classification', model='google-bert/bert-base-uncased')\n",
    "classifier_fine_tuned = pipeline('text-classification', model='vincentclaes/autotrain-0br8k-gdjpm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54db095c7a6b831a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T09:01:58.625566Z",
     "start_time": "2024-09-06T08:59:04.842662Z"
    }
   },
   "outputs": [],
   "source": [
    "# read test.csv, and make prediction for the column 'question' and add the result to the column 'predicted_class_name'\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "LABELS = [\"ACCOUNT\", \"CANCEL\", \"CONTACT\", \"DELIVERY\", \"FEEDBACK\", \"INVOICE\", \"ORDER\", \"PAYMENT\", \"REFUND\", \"SHIPPING\", \"SUBSCRIPTION\"]\n",
    "df_zero_shot = df.copy()\n",
    "df_zero_shot['predicted_class_name'] = df_zero_shot['question'].apply(lambda x: classifier_zero_shot(x, candidate_labels=LABELS)[\"labels\"][0])\n",
    "\n",
    "df_fine_tuned = df.copy()\n",
    "df_fine_tuned['predicted_class_name'] = df_fine_tuned['question'].apply(lambda x: classifier_fine_tuned(x)[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bac7ca9bf7b74e82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T09:04:16.105578Z",
     "start_time": "2024-09-06T09:04:15.111400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Accuracy: {'accuracy': 0.02727272727272727}\n",
      "Fine-Tuned Accuracy: {'accuracy': 0.996969696969697}\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(df):\n",
    "    # use the huggingface evaluate library to evaluate the model by taking the columns\n",
    "    # 'predicted_class_name' and 'class_name' as input andd calculate the accuracy\n",
    "    from datasets import load_metric\n",
    "    # Create a mapping of class names to numerical labels\n",
    "    unique_classes = set(df['predicted_class_name']).union(set(df['class_name']))\n",
    "    class_to_int = {cls_name: idx for idx, cls_name in enumerate(unique_classes)}\n",
    "    \n",
    "    # Map the class names to integers\n",
    "    df['predicted_class_numeric'] = df['predicted_class_name'].map(class_to_int)\n",
    "    df['class_numeric'] = df['class_name'].map(class_to_int)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    accuracy = metric.compute(predictions=df['predicted_class_numeric'], references=df['class_numeric'])\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "zero_shot_acc = calculate_accuracy(df_zero_shot)\n",
    "print(f\"Zero-Shot Accuracy: {zero_shot_acc}\")\n",
    "fine_tuned_acc = calculate_accuracy(df_fine_tuned)\n",
    "print(f\"Fine-Tuned Accuracy: {fine_tuned_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
